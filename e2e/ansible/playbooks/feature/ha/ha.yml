# ha.yml
#Description: Including libiscsi compliance test to OpenEBS e2e suite.

###############################################################################################
#Test Steps:
#1. Install the packages and dependencies.
#2. Copy the test artifacts to k8s master.
#3. Check if the OpenEBS components are deployed.
#3. Deploy Percona application.
#4. Check if the pods are up and running.
#5. Obtain the node where the conteoller pod is scheduled.
#6. Make the current node unschedulable.
#7. Delete the controller pod anc check if it is scheduled again on different node.
#7. Check if the application pod runs seamlessly.
#8. Perform test cleanup.
###############################################################################################

---
- hosts: localhost

  vars_files:
    - ha-vars.yml

  tasks:
   - block:

       #################################################
       # PREPARE TEST ENV FOR THE HA TEST              #
       # (Includes installing packages, deploying apps)#
       #################################################

       - include: ha-prerequisites.yml

       - include_tasks: "{{utils_path}}/deploy_check.yml"
         vars:
            ns: "{{ operator_ns }}"
            lkey: name
            lvalue: maya-apiserver

       - name: Download YAML for percona mysql plugin
         get_url:
           url: "{{ percona_mysql_plugin_link }}"
           dest: "{{ result_kube_home.stdout }}/{{ pod_yaml_alias }}"
           force: yes
         register: result
         until:  "'OK' in result.msg"
         delay: 5
         retries: 3
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Replace storage-class to use cstor storage engine
         replace:
           path: "{{ result_kube_home.stdout }}/{{ pod_yaml_alias }}"
           regexp: 'openebs-standard'
           replace: '{{ cstor_sc }}'
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"
         when:  storage_engine  == 'cStor'

       - name: Replace storage-class to use jiva storage engine
         replace:
           path: "{{ result_kube_home.stdout }}/{{ pod_yaml_alias }}"
           regexp: 'openebs-standard'
           replace: '{{ jiva_sc }}'
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"
         when:  storage_engine  == 'jiva'

       - name: Create namespaces.  
         include_tasks: "{{utils_path}}/namespace_task.yml"
         vars:
           status: create
           ns: "{{ namespace }}"

       - include_tasks: "{{utils_path}}/deploy_task.yml"
         vars:
           app_yml: "{{ pod_yaml_alias }}"
           ns: "{{ namespace }}"

       - name: Confirm pod status is running
         include_tasks: "{{utils_path}}/deploy_check.yml"
         vars:
          ns: "{{ namespace }}"
          lkey: name
          lvalue: percona
       ###################################################################
       # PREPARE FOR FAULT-INJECTION                                     #
       # (Includes identifying objects to fail, simulating infra changes)#
       ###################################################################

       - name: Get storage ctrl pod name
         shell: >
           source ~/.profile;
           kubectl get pods -n {{ namespace }} -l openebs/controller=jiva-controller
           --no-headers
         args:
           executable: /bin/bash
         register: result
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"
         when: storage_engine == 'jiva'

       - name: Get storage ctrl pod name
         shell: >
           source ~/.profile;
           kubectl get pods -n {{ operator_ns }} -l openebs.io/controller=cstor-controller | grep {{ namespace }}
         args:
           executable: /bin/bash
         register: result
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"
         when: storage_engine == 'cStor'

       - name: Set ctrl pod name to variable
         set_fact:
           ctrl_pod_name: "{{ result.stdout.split()[0] }}"

       - name: Get node on which the pod is scheduled
         shell: >
           source ~/.profile;
           kubectl get pods -n {{ namespace }} -o wide | grep {{ctrl_pod_name}} | awk {'print $7'}
         args:
           executable: /bin/bash
         register: result
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"
         when: storage_engine == 'jiva'

       - name: Get node on which the pod is scheduled
         shell: >
           source ~/.profile;
           kubectl get pods -n {{ operator_ns }} -o wide | grep {{ctrl_pod_name}} | awk {'print $7'}
         args:
           executable: /bin/bash
         register: result
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"
         when: storage_engine == 'cStor'

       - name: Store the ctrl pod node name in a variable
         set_fact:
           ctrl_node: "{{ result.stdout }}"

       - name: Make the current ctrl node unschedulable
         shell: source ~/.profile; kubectl cordon {{ctrl_node}}
         args:
           executable: /bin/bash
         register: result
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Confirm ctrl node is unschedulable
         shell: source ~/.profile; kubectl get nodes  | grep {{ctrl_node}}
         args:
           executable: /bin/bash
         register: result
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         failed_when: "'SchedulingDisabled' not in result.stdout"

       ########################################################
       # INJECT FAULT                                         #
       # (Includes using tools, commands etc.,)               #
       ########################################################

       - name: Delete the ctrl pod to force reschedule
         shell: source ~/.profile; kubectl delete pod {{ctrl_pod_name}} -n {{ namespace }}
         args:
           executable: /bin/bash
         register: result
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         failed_when: "ctrl_pod_name and 'deleted' not in result.stdout"
         when: storage_engine == 'jiva'

       - name: Delete the ctrl pod to force reschedule
         shell: source ~/.profile; kubectl delete pod {{ctrl_pod_name}} -n {{ operator_ns }}
         args:
           executable: /bin/bash
         register: result
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         failed_when: "ctrl_pod_name and 'deleted' not in result.stdout"
         when: storage_engine == 'cStor'

       #########################################################
       # VERIFY RECOVERY                                       #
       # (Includes checking desired fault-tolerance behaviour) #
       #########################################################

       - name: Check if ctrl pod is restarted within 30s
         shell: >
           source ~/.profile;
           kubectl get pods -n {{ namespace }} -l openebs/controller=jiva-controller
         args:
           executable: /bin/bash
         register: result
         until: "'Running' in result.stdout_lines[1]"
         delay: 15
         retries: 6
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"
         when: storage_engine == 'jiva'

       - name: Check if ctrl pod is restarted within 30s
         shell: >
           source ~/.profile;
           kubectl get pods -n {{ operator_ns }} -l openebs.io/controller=cstor-controller | grep {{ namespace }}
         args:
           executable: /bin/bash
         register: result
         until: "'Running' in result.stdout_lines[1]"
         delay: 15
         retries: 6
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"
         when: storage_engine == 'cStor'

       #######################################################
       # POST RECOVERY HEALTH-CHECK                          #
       # (Includes application status etc.,)                 #
       #######################################################

       - name: Get percona pod details post ha
         shell: >
           source ~/.profile;
           kubectl get pods -n {{ namespace }} -l name=percona -o wide  --no-headers
         args:
           executable: /bin/bash
         register: result
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Set pod variables to facts
         set_fact:
           pod_status: "{{ result.stdout.split()[2] }}"
           pod_restarts: "{{ result.stdout.split()[3] }}"
           pod_ip: "{{ result.stdout.split()[5] }}"
           pod_name: "{{ result.stdout.split()[1] }}"

       - name: Verify percona is running w/o restarts
         debug:
           msg: "Percona pod health-check is successful"
         failed_when: "pod_status != 'Running' and pod_restarts != '0'"

       - name: Create a test db on percona to confirm health
         shell: |
           sleep 100
           kubectl exec -it {{ pod_name }} -n {{ namespace }} -- mysql -uroot -pk8sDem0 -e "create database tdb;"
           kubectl exec -it {{ pod_name }} -n {{ namespace }} -- mysql -uroot -pk8sDem0 -e "create table ttbl (Data VARCHAR(20));" tdb
           kubectl exec -it {{ pod_name }} -n {{ namespace }} -- mysql -uroot -pk8sDem0 -e "insert into ttbl (Data) VALUES ('tdata');" tdb
           kubectl exec -it {{ pod_name }} -n {{ namespace }} -- mysql -uroot -pk8sDem0 -e "flush tables with read lock;"
         args:
           executable: /bin/bash
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         register: result
         failed_when: "result.rc != 0"

       ########################################################
       # REVERT FAULTS AND CLEANUP                            #
       # (Includes commands, delete applications etc.,)       #
       ########################################################

       - name: Uncordon the K8S node as part of cleanup
         shell: source ~/.profile; kubectl uncordon {{ctrl_node}}
         args:
           executable: /bin/bash
         register: result
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Setting pass flag
         set_fact:
            flag: "Test Passed"
            status: "good"
            status_id: 1

     rescue:
       - name: Setting fail flag
         set_fact:
           flag: "Test Failed"
           status: "danger"
           status_id: 5

     always:
       - block:

           - include: ha-cleanup.yml

           - name: Test Cleanup Passed
             set_fact:
               cflag: "Cleanup Passed"

         rescue:
           - name: Test Cleanup Failed
             set_fact:
               cflag: "Cleanup Failed"

         always:

           - include_tasks: "{{utils_path}}/stern_task.yml"
             vars:
               status: stop

           - include_tasks: "{{utils_path}}/namespace_task.yml"
             vars:
               status: delete
               ns: "{{ namespace }}"

           - include_tasks: "{{utils_path}}/update-testrail.yml"
             vars:
               user: "{{ testrail_user }}"
               password: "{{ testrail_password }}"
               suite_id: "{{ test_suite_id }}"
               case_id: "{{ test_case_id }}"
               st_id: "{{ status_id }}"

           - name: Send slack notification
             slack:
              token: "{{ lookup('env','SLACK_TOKEN') }}"
              msg: '{{ ansible_date_time.time }} TEST: {{test_name}}, RESULT: {{ flag }},{{cflag}},{{tflag}}'
              color: "{{status}}"
             when: slack_notify | bool and lookup('env','SLACK_TOKEN')
