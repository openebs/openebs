#Description: Verify behaviour of "kubectl drain" with OpenEBS volume replicas running on Nodes
#kubectl drain <node>: is one of the common procedures used to implement maintenance cycles on the nodes (hardware refresh, etc..,). With NodeAffinity/Stickiness, verify if is drain is successful.
#Author: Swarna
########################################################################################################################################################################
#Steps:
#1.Deploy openebs operator and storage classes.
#2.Check the maya-apiserver and openebs-provisioner are running
#3.Deploy percona application
#4.Check if the application and the volume pods are up and running.
#5.Check on which node replicas are running anf get the node names.
#6.Update the node names in patch.yaml.
#7.Patch the deployment to include node affinity spec.
#8.Drain the node on which replicas are running using "kubectl drain <nodename>" and check replicas are going to pending state.
#9.Perform cleanup.
########################################################################################################################################################################


- hosts: localhost

  vars_files:
    - kubectl-drain-vars.yml

  tasks:
   - block:


       - include: pre-requisites.yml

       - name: Get the Number of nodes count
         shell: source {{ profile }}; kubectl get nodes | grep 'Ready' | grep 'none' | wc -l
         args:
           executable: /bin/bash
         register: node_out
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Fetch the node count from stdout
         set_fact:
            node_count: " {{ node_out.stdout}}"


       - name: Check status of maya-apiserver
         include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/deploy_check.yml"
         vars:
           ns: "{{ operator_ns }}"
           app: maya-apiserver


       - name: Obtaining storage classes yaml
         shell: source ~/.profile; kubectl get sc openebs-percona -o yaml > "{{ create_sc }}"
         args:
           executable: /bin/bash
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Replace the replica count in openebs-storageclasses yaml
         replace:
           path: "{{ create_sc }}"
           regexp: 'openebs.io/jiva-replica-count: "3"'
           replace: 'openebs.io/jiva-replica-count: "{{ (node_count) |int-1 }}"'
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Delete the existing storage class and create new one
         shell: source ~/.profile; kubectl delete sc openebs-percona; kubectl apply -f "{{ create_sc }}"
         args:
           executable: /bin/bash
         register: sc_out
         until: "'created' in sc_out.stdout"
         delay: 10
         retries: 5
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Create namespace to deploy application
         shell: source ~/.profile; kubectl create ns {{ namespace }}
         args:
           executable: /bin/bash
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"


       - name: Get percona spec and liveness scripts
         get_url:
           url: "{{ percona_link }}"
           dest: "{{ result_kube_home.stdout }}"
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

       - name: Deploy percona application
         shell: source {{ profile }}; kubectl apply -f {{ percona_link }} -n {{ namespace }}
         args:
           executable: /bin/bash
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Check percona pod is running
         include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/deploy_check.yml"
         vars:
           ns: "{{namespace}}"
           app: percona


       - name: Check if the replica pods are created and running
         shell: source ~/.profile; kubectl get pods -n {{ namespace }} | grep rep | grep -i running |wc -l
         args:
           executable: /bin/bash
         register: rep_count
         until: "'2' in rep_count.stdout"
         delay: 60
         retries: 5
         ignore_errors: true
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"


       - name: Get PV name
         shell: source ~/.profile; kubectl get pv -n {{ namespace }}| grep openebs-percona | awk '{print $1}'
         args:
           executable: /bin/bash
         register: pv_name
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"


       - name: Copy replica_patch.yaml to master
         copy:
           src: "{{ patch_file }}"
           dest: "{{ result_kube_home.stdout }}"
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Get the node name where replica1 is scheduled
         shell: source ~/.profile; kubectl get po -n {{ namespace }} -o wide | grep "{{ pv_name.stdout }}" | grep rep | awk '{print $7}' | awk 'FNR == 1 {print}'
         args:
           executable: /bin/bash
         register: node1
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"


       - name: Get the node name where replica2 is scheduled
         shell: source ~/.profile; kubectl get po -n {{ namespace }} -o wide | grep "{{ pv_name.stdout }}" | grep rep | awk '{print $7}' | awk 'FNR == 2 {print}'
         args:
           executable: /bin/bash
         register: node2
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"


       - name: Update replica1 node name in replica_patch file
         replace:
           path: "{{ result_kube_home.stdout }}/{{ patch_file }}"
           regexp: 'nodename_where_replica_pod_1_got_scheduled'
           replace: '{{ node1.stdout }}'
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"


       - name: Update replica2 node name in replica_patch file
         replace:
           path: "{{ result_kube_home.stdout }}/{{ patch_file }}"
           regexp: 'nodename_where_replica_pod_2_got_scheduled'
           replace: '{{ node2.stdout }}'
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"


       - name: Get the deployment name
         shell: source ~/.profile; kubectl get deploy -n {{ namespace }} |grep "{{ pv_name.stdout }}" | grep rep |awk '{print $1}'
         args:
           executable: /bin/bash
         register: deploy_name
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"


       - name: Apply the node affinity property
         shell: source ~/.profile; kubectl patch deployment "{{ deploy_name.stdout }}" -n {{ namespace }} -p "$(cat replica_patch.yaml)"
         args:
           executable: /bin/bash
         register: patch_out
         until: "'patched' in patch_out.stdout"
         delay: 20
         retries: 5
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"


       - name: Drain the node where replica is scheduled
         shell: source ~/.profile; kubectl drain {{ node1.stdout }} --ignore-daemonsets --force
         args:
           executable: /bin/bash
         register: result
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Check the available replicas in deployment
         shell: source ~/.profile; kubectl get deploy -n {{ namespace }} | grep "{{ pv_name.stdout }}" | grep rep | awk '{print $2}'
         args:
           executable: /bin/bash
         register: available_pods
         until: "'2' in available_pods.stdout"
         delay: 30
         retries: 5
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         ignore_errors: true


       - name: Check if the replica is in pending state after node drain
         shell: source ~/.profile; kubectl get pods -n {{ namespace }} -o wide | grep "{{ pv_name.stdout }}" | grep rep | grep Pending
         args:
           executable: /bin/bash
         register: result
         until: "'Pending' in result.stdout"
         delay: 30
         retries: 15
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Test playbook passed
         set_fact:
           flag: "Test Passed"
           status_id: 1

       - name: Set Status
         set_fact:
           status: "good"

     rescue:
       - name: Test playbook failed
         set_fact:
           flag: "Test Failed"
           status_id: 5

       - name: Set Status
         set_fact:
           status: "danger"

     always:
       - block:

           - include: cleanup.yml
             when: clean | bool

           - name: Test Cleanup Passed
             set_fact:
               cflag: "Cleanup Passed"

         rescue:
           - name: Test Cleanup Failed
             set_fact:
               cflag: "Cleanup Failed"

         always:

           - include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/stern_task.yml"
             vars:
               status: stop

           - name: Update Testrail
             shell: source ~/.profile; python3 {{ inventory_dir | dirname }}/files/updater.py -tuser {{ testrail_user }} -tpass {{ testrail_password }} -sid {{ test_suite_id }} -cid {{ test_case_id }} -stid {{ status_id }}
             args:
               executable: /bin/bash

           - name: Testrail Updated
             set_fact:
               tflag: "Testrail Updated"

         rescue:
           - name: Testrail Update failed
             set_fact:
               tflag: "Testrail Update Failed"

           - name: Send slack notification
             slack:
               token: "{{ lookup('env','SLACK_TOKEN') }}"
               msg: '{{ ansible_date_time.time }} TEST: {{test_name}}, RESULT: {{ flag }},{{ cflag }},{{tflag}}'
               color: "{{status}}"
             when: slack_notify | bool and lookup('env','SLACK_TOKEN')



