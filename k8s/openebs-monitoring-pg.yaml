# The following file is intended for deployments that are not already
# configured with prometheus. This is a minified version of the config
# from the files present under ./openebs-monitoring/
# 
# Prometheus tunables
apiVersion: v1
kind: ConfigMap
metadata:
  name: openebs-prometheus-tunables
  namespace: openebs
data:
  storage-retention: 24h
---
# Define the openebs prometheus jobs
kind: ConfigMap
metadata:
  name: openebs-prometheus-config
  namespace: openebs
apiVersion: v1
data:
  prometheus.yml: |-
    global:
      external_labels:
        slave: slave1
      scrape_interval: 5s
      evaluation_interval: 5s
    rule_files:
      - "/etc/prometheus-rules/*.rules"
    alerting:
      alertmanagers:
        - scheme: http
          path_prefix: /
          static_configs:
            - targets: ['alertmanager:9093']
    scrape_configs:
    - job_name: 'prometheus'
      scheme: http
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_name]
        regex: openebs-prometheus-server
        action: keep
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name
    - job_name: 'openebs-volumes'
      scheme: http
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_monitoring]
        regex: volume_exporter_prometheus
        action: keep
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name
      # Below entry ending with vsm is deprecated and is maintained for
      # backward compatibility purpose.
      - source_labels: [__meta_kubernetes_pod_label_vsm]
        action: replace
        target_label: openebs_pv
      # Below entry is the correct entry. Though the above and below entries
      # are having same target_label as openebs_pv, only one of them will be
      # valid for any release.
      - source_labels: [__meta_kubernetes_pod_label_openebs_io_persistent_volume]
        action: replace
        target_label: openebs_pv
      - source_labels: [__meta_kubernetes_pod_container_port_number]
        action: drop
        regex: '(.*)9501'
      - source_labels: [__meta_kubernetes_pod_container_port_number]
        action: drop
        regex: '(.*)3260'
      - source_labels: [__meta_kubernetes_pod_container_port_number]
        action: drop
        regex: '(.*)80'
    - job_name: 'openebs-pools'
      scheme: http
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_openebs_io_monitoring]
        regex: pool_exporter_prometheus
        action: keep
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name
      - source_labels: [__meta_kubernetes_pod_label_openebs_io_storage_pool_claim]
        action: replace
        target_label: storage_pool_claim
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: ${1}:${2}
        target_label: __address__
    - job_name: 'node'
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - source_labels: [__meta_kubernetes_role]
        action: replace
        target_label: kubernetes_role
      - source_labels: [__address__]
        regex: '(.*):10250'
        replacement: '${1}:9100'
        target_label: __address__
      - source_labels: [__meta_kubernetes_node_label_kubernetes_io_hostname]
        target_label: __instance__
      - source_labels: [job]
        regex: 'kubernetes-(.*)'
        replacement: '${1}'
        target_label: name
    - job_name: 'mysqld'
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        regex: prometheus-mysql-exporter
        action: keep
    - job_name: 'kubernetes-nodes-cadvisor'
      scrape_interval: 10s
      scrape_timeout: 10s
      scheme: https  # remove if you want to scrape metrics on insecure port
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
        - role: node
      relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        # Only for Kubernetes ^1.7.3.
        # See: https://github.com/prometheus/prometheus/issues/2916
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
      metric_relabel_configs:
        - action: replace
          source_labels: [id]
          regex: '^/machine\.slice/machine-rkt\\x2d([^\\]+)\\.+/([^/]+)\.service$'
          target_label: rkt_container_name
          replacement: '${2}-${1}'
        - action: replace
          source_labels: [id]
          regex: '^/system\.slice/(.+)\.service$'
          target_label: systemd_service_name
          replacement: '${1}'
    - job_name: 'kube-state-metrics'
      static_configs:
        - targets: ['kube-state-metrics.openebs.svc.cluster.local:8080']
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: openebs-prometheus-rules
  labels:
    name: openebs-prometheus-rules
  namespace: openebs
data:
  alert.rules: |-
    groups:
      - name: CPU 
        rules:
        - alert: High CPU Load
          expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
          for: 1m
          labels:
            team: devops
          annotations:
            summary: "High CPU load (instance {{ $labels.instance }})"
            description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - name: Memory
        rules:
        - alert: High Memory Utiliation
          expr: (node_memory_MemFree_bytes + node_memory_Cached_bytes + node_memory_Buffers_bytes) / node_memory_MemTotal_bytes * 100 < 15
          for: 1m
          labels:
            team: devops
          annotations:
            summary: "Out of Memory (instance {{ $labels.instance }})" 
            description: "Memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - name: Filesystem
        rules:
        - alert: No Root Disk Space Left
          expr: node_filesystem_free_bytes{mountpoint ="/"} / node_filesystem_size_bytes{mountpoint ="/"} * 100 < 10
          for: 1m
          labels:
            team: devops
          annotations:
            summary: "Out of disk root space (instance {{ $labels.instance }})" 
            description: "Root Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        - alert: No Mounted Disk Space Left
          expr: node_filesystem_free_bytes{mountpoint !="/"} / node_filesystem_size_bytes{mountpoint !="/"} * 100 < 10
          for: 1m
          labels:
            team: devops
          annotations:
            summary: "Out of mounted disk space (instance {{ $labels.instance }})" 
            description: "Mounted Disk is almost full (< 10% left) \n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - name: Kubernetes
        rules:
        - alert: Pod CrashLoopBackOff 
          expr: kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff"} > 0
          for: 1m
          labels:
            team: devops
          annotations:
            summary: "Pod '{{$labels.pod}}' in namespace '{{$labels.namespace}}' is in CrashLoopBackOff" 
            description: "A container named '{{$labels.container}}' in the pod '{{$labels.pod}}' in namespace '{{$labels.namespace}}' is experiencing restarts"
            
      - name: OpenEBS
        rules:
        - alert: OpenEBS Volume Not Available
          expr: openebs_volume_status == 1 or openebs_volume_status == 4
          for: 1m
          labels:
            team: devops
          annotations:
            summary: "Volume '{{ $labels.openebs_pv }}' created for claim '{{ $labels.openebs_pvc }}' is not available"
            description: "Volume '{{ $labels.openebs_pv }}' if offline, either because replica quorum is not met, target is not running or backend storage is lost"
---
# prometheus-deployment
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: openebs-prometheus
  namespace: openebs
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: openebs-prometheus-server
    spec:
      serviceAccountName: openebs-maya-operator
      containers:
        - name: prometheus
          image: prom/prometheus:v2.11.0
          args:
            - "--config.file=/etc/prometheus/conf/prometheus.yml"
            # Metrics are stored in an emptyDir volume which
            # exists as long as the Pod is running on that Node.
            # The data in an emptyDir volume is safe across container crashes.
            - "--storage.tsdb.path=/prometheus"
            # How long to retain samples in the local storage.
            - "--storage.tsdb.retention=$(STORAGE_RETENTION)"
          ports:
            - containerPort: 9090
          env:
            # environment vars are stored in prometheus-env configmap. 
            - name: STORAGE_RETENTION
              valueFrom:
                configMapKeyRef:
                  name: openebs-prometheus-tunables
                  key: storage-retention
          resources:
            requests:
              # A memory request of 250M means it will try to ensure minimum
              # 250MB RAM .
              memory: "128M"
              # A cpu request of 128m means it will try to ensure minimum
              # .125 CPU; where 1 CPU means :
              # 1 *Hyperthread* on a bare-metal Intel processor with Hyperthreading
              cpu: "128m"
            limits:
              memory: "700M"
              cpu: "500m"
          
          volumeMounts:
            # prometheus config file stored in the given mountpath
            - name: prometheus-server-volume
              mountPath: /etc/prometheus/conf
            # metrics collected by prometheus will be stored at the given mountpath.
            - name: prometheus-storage-volume
              mountPath: /prometheus
            - name: prometheus-rules-volume
              mountPath: /etc/prometheus-rules
      volumes:
        # Prometheus Config file will be stored in this volume 
        - name: prometheus-server-volume
          configMap:
            name: openebs-prometheus-config
        # Alert rules will be storesin this volume
        - name: prometheus-rules-volume
          configMap:
            name: openebs-prometheus-rules
        # All the time series stored in this volume in form of .db file.
        - name: prometheus-storage-volume
          # containers in the Pod can all read and write the same files here.
          emptyDir: {} 
---
# prometheus-service
apiVersion: v1
kind: Service
metadata:
  name: openebs-prometheus-service
  namespace: openebs
spec:
  selector: 
    name: openebs-prometheus-server
  type: NodePort
  ports:
    - port: 80 # this Service's port (cluster-internal IP clusterIP)
      targetPort: 9090 # pods expose this port
      nodePort: 32514
      # Note that this Service will be visible as both NodeIP:nodePort and clusterIp:Port
---
apiVersion: v1
kind: Service
metadata:
  name: openebs-grafana
  namespace: openebs
spec:
  type: NodePort
  ports:
  - port: 3000
    targetPort: 3000
    nodePort: 32515
  selector:
    app: openebs-grafana
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: openebs-grafana
  name: openebs-grafana
  namespace: openebs
spec:
  replicas: 1
  revisionHistoryLimit: 2
  template:
    metadata:
      labels:
        app: openebs-grafana
    spec:
      containers:
      - image: grafana/grafana:6.3.0
        name: grafana
        ports:
        - containerPort: 3000
        env:
          - name: GF_AUTH_BASIC_ENABLED
            value: "true"
          - name: GF_AUTH_ANONYMOUS_ENABLED
            value: "false"
        livenessProbe:
          httpGet:
            path: /
            port: 3000
          initialDelaySeconds: 30
          timeoutSeconds: 1
---
# node-exporter will be launch as daemonset.
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: openebs
spec:
  template:
    metadata:
      labels:
        app: node-exporter
      name: node-exporter
    spec:
      containers:
      #- image: prom/node-exporter:v0.18.1
      - image: quay.io/prometheus/node-exporter:v0.18.1 
        args:
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --path.rootfs=/host/root
          - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib|run|boot|home/kubernetes/.+)($|/)
          - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$
        name: node-exporter
        ports:
        - containerPort: 9100
          hostPort: 9100
          name: scrape
        resources:
            requests:
              # A memory request of 250M means it will try to ensure minimum
              # 250MB RAM .
              memory: "128M"
              # A cpu request of 128m means it will try to ensure minimum
              # .125 CPU; where 1 CPU means :
              # 1 *Hyperthread* on a bare-metal Intel processor with Hyperthreading
              cpu: "128m"
            limits:
              memory: "700M"
              cpu: "500m"
        volumeMounts:
        # All the application data stored in data-disk
        - name: proc
          mountPath: /host/proc
          readOnly: false
        # Root disk is where OS(Node) is installed
        - name: sys
          mountPath: /host/sys
          readOnly: false
        - name: root
          mountPath: /host/root
          mountPropagation: HostToContainer
          readOnly: true
      # The Kubernetes scheduler’s default behavior works well for most cases
      # -- for example, it ensures that pods are only placed on nodes that have 
      # sufficient free resources, it ties to spread pods from the same set 
      # (ReplicaSet, StatefulSet, etc.) across nodes, it tries to balance out 
      # the resource utilization of nodes, etc.
      #
      # But sometimes you want to control how your pods are scheduled. For example,
      # perhaps you want to ensure that certain pods only schedule on nodes with 
      # specialized hardware, or you want to co-locate services that communicate 
      # frequently, or you want to dedicate a set of nodes to a particular set of 
      # users. Ultimately, you know much more about how your applications should be
      # scheduled and deployed than Kubernetes ever will.
      #
      # “taints and tolerations,” allows you to mark (“taint”) a node so that no 
      # pods can schedule onto it unless a pod explicitly “tolerates” the taint.
      # toleration  is particularly useful for situations where most pods in 
      # the cluster should avoid scheduling onto the node. In our case we want
      # node-exporter to run on master node also i.e, we want to collect metrics 
      # from master node. That's why tolerations added.
      # if removed master's node metrics can't be scrapped by prometheus.
      tolerations:
      - effect: NoSchedule
        operator: Exists
      volumes:
        # A hostPath volume mounts a file or directory from the host node’s 
        # filesystem.For example, some uses for a hostPath are:
        # running a container that needs access to Docker internals; use a hostPath 
        # of /var/lib/docker
        # running cAdvisor in a container; use a hostPath of /dev/cgroups
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: root
          hostPath:
            path: /
      hostNetwork: true
      hostPID: true
